{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dadAJEp9wc0d"
   },
   "outputs": [],
   "source": [
    "# Q1: Demo matrices - 1 point\n",
    "\n",
    "# Assuming a Stride of 1, the Output matrix will be:\n",
    "# 4 3 4\n",
    "# 2 4 3\n",
    "# 2 3 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lanYgQqZ09EG"
   },
   "source": [
    "Calculate the output:\n",
    "\n",
    "Image:\n",
    "\n",
    "1 1 1 0 0\n",
    "\n",
    "0 1 1 1 0\n",
    "\n",
    "0 0 1 1 1\n",
    "\n",
    "0 0 1 1 0\n",
    "\n",
    "0 1 1 0 0\n",
    "\n",
    "\n",
    "Filter:\n",
    "\n",
    "1 0 1\n",
    "\n",
    "0 1 0\n",
    "\n",
    "1 0 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jZkrCg5b1IHk"
   },
   "outputs": [],
   "source": [
    "#Q2: What are the advantages of a CNN over a fully connected DNN for image classification? - 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- They are able to capture and learn relevant features from an image at different levels while a DNN cannot (feature learning)\n",
    "- CNN networks can be less complex than DNN through the use of filters and weight sharing: each neuron does not need to be fully connected\n",
    "- This lowering of neuron connections allow the CNN to be trained much faster than a DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nPDL4oYRxFDC"
   },
   "outputs": [],
   "source": [
    "#Q3: If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem? - 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Try to rescale the images down in size so that they don't take up that much memory\n",
    "- 2) Lower the batch size for training so that fewer images are trained during each batch\n",
    "- 3) Decrease the number of layers within the CNN itself so that fewer parameters are being used\n",
    "- 4) Use Pooling layers within the network to downsample the image to a smaller size\n",
    "- 5) Buy a better GPU :) - Kidding - Adjust the filter output size so that it's smaller than the current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hNftAEsdxMMQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 13s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Q4: Use the dataset: https://www.tensorflow.org/datasets/catalog/citrus_leaves - 1 point\n",
    "# Using the Cifar10 dataset since citrus leaves has been not working\n",
    "import tensorflow as tf\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "#Q5: Split it into a training set, and a test set. - 1 point\n",
    "# Above dataset is already loaded as train and test data with 50,000 images for train\n",
    "# and 10,000 for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ozhxqfTkxU4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                108170    \n",
      "=================================================================\n",
      "Total params: 127,562\n",
      "Trainable params: 127,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Q6: Build a CNN model - 3 points\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "# Adding a 2D convolution layer and specifying the input shape of the matrix\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "# Max pool layer for downsizing and specifying size of output\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# Another layer - specifying neuron size and matrix output\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# Flattening 3D image down to 1D for classification\n",
    "model.add(layers.Flatten())\n",
    "#model.add(layers.Dense(64, activation='relu'))\n",
    "# Adding dense layer as final output for classes\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Adam optimizer for fast training and SparseCatCross for image loss calculation\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "# Credit to: https://www.tensorflow.org/tutorials/images/cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 2.2286 - accuracy: 0.3204 - val_loss: 1.5778 - val_accuracy: 0.4322\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4369 - accuracy: 0.4932 - val_loss: 1.4633 - val_accuracy: 0.4816\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2720 - accuracy: 0.5600 - val_loss: 1.3106 - val_accuracy: 0.5440\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.1061 - accuracy: 0.6180 - val_loss: 1.2950 - val_accuracy: 0.5608\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.9915 - accuracy: 0.6602 - val_loss: 1.3333 - val_accuracy: 0.5762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9cdf1a2790>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3863 - accuracy: 0.5672\n",
      "Model accuracy:  0.5672000050544739\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels)\n",
    "print(\"Model accuracy: \",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mWCCCfgmxY45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 13, 13, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 11, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 9, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                102464    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 271,242\n",
      "Trainable params: 270,730\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Q7: Fine-tune it and learn its accuracy. Bring the minimum accuracy of 85%. Jot down any challenges/issues you face during this process. - 3 points\n",
    "\n",
    "model = models.Sequential()\n",
    "# Adding a 2D convolution layer and specifying the input shape of the matrix\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "# Max pool layer for downsizing and specifying size of output\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# Another layer - specifying neuron size and matrix output\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Flattening 3D image down to 1D for classification\n",
    "model.add(layers.Flatten())\n",
    "# Adding hidden dense layer before output layer\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# Adding dense layer as final output for classes\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()\n",
    "# Adam optimizer for fast training and SparseCatCross for image loss calculation\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1407/1407 [==============================] - 63s 45ms/step - loss: 1.5499 - accuracy: 0.4404 - val_loss: 1.6341 - val_accuracy: 0.4422\n",
      "Epoch 2/15\n",
      "1407/1407 [==============================] - 60s 43ms/step - loss: 1.1887 - accuracy: 0.5786 - val_loss: 1.1929 - val_accuracy: 0.5632\n",
      "Epoch 3/15\n",
      "1407/1407 [==============================] - 66s 47ms/step - loss: 1.0209 - accuracy: 0.6420 - val_loss: 1.0038 - val_accuracy: 0.6478\n",
      "Epoch 4/15\n",
      "1407/1407 [==============================] - 67s 48ms/step - loss: 0.9016 - accuracy: 0.6818 - val_loss: 0.9853 - val_accuracy: 0.6578\n",
      "Epoch 5/15\n",
      "1407/1407 [==============================] - 62s 44ms/step - loss: 0.8074 - accuracy: 0.7177 - val_loss: 0.9998 - val_accuracy: 0.6572\n",
      "Epoch 6/15\n",
      "1407/1407 [==============================] - 62s 44ms/step - loss: 0.7258 - accuracy: 0.7434 - val_loss: 1.0821 - val_accuracy: 0.6240\n",
      "Epoch 7/15\n",
      "1407/1407 [==============================] - 66s 47ms/step - loss: 0.6612 - accuracy: 0.7684 - val_loss: 1.1956 - val_accuracy: 0.6094\n",
      "Epoch 8/15\n",
      "1407/1407 [==============================] - 63s 44ms/step - loss: 0.6032 - accuracy: 0.7909 - val_loss: 0.8821 - val_accuracy: 0.7046\n",
      "Epoch 9/15\n",
      "1407/1407 [==============================] - 68s 49ms/step - loss: 0.5563 - accuracy: 0.8048 - val_loss: 0.8058 - val_accuracy: 0.7324\n",
      "Epoch 10/15\n",
      "1407/1407 [==============================] - 61s 44ms/step - loss: 0.5073 - accuracy: 0.8213 - val_loss: 0.9005 - val_accuracy: 0.7170\n",
      "Epoch 11/15\n",
      "1407/1407 [==============================] - 66s 47ms/step - loss: 0.4690 - accuracy: 0.8348 - val_loss: 0.9062 - val_accuracy: 0.7286\n",
      "Epoch 12/15\n",
      "1407/1407 [==============================] - 66s 47ms/step - loss: 0.4287 - accuracy: 0.8480 - val_loss: 0.8845 - val_accuracy: 0.7314\n",
      "Epoch 13/15\n",
      "1407/1407 [==============================] - 65s 46ms/step - loss: 0.3937 - accuracy: 0.8603 - val_loss: 0.8600 - val_accuracy: 0.7358\n",
      "Epoch 14/15\n",
      "1407/1407 [==============================] - 64s 45ms/step - loss: 0.3597 - accuracy: 0.8726 - val_loss: 0.9018 - val_accuracy: 0.7312\n",
      "Epoch 15/15\n",
      "1407/1407 [==============================] - 62s 44ms/step - loss: 0.3285 - accuracy: 0.8850 - val_loss: 1.1822 - val_accuracy: 0.6958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21a99b83400>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=15, validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 8ms/step - loss: 1.2090 - accuracy: 0.6863\n",
      "Model accuracy:  0.6862999796867371\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels)\n",
    "print(\"Model accuracy: \",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges/Process to 85%\n",
    "- First iteration was base model at Q6. Accuracy was low for training and even lower for testing (56%). Attempt to add more convolution layers and train for more epochs\n",
    "- Second attempt was slightly better by adding two more convolution layers to model. Test accuracy at 60%. Will try adding dense layer after the flatten\n",
    "- Adding extra dense layer actually made accuracy drop to 58%. Going to normalize the data to between 0 and 1 and add more convolution layers to model. Also running for 15 epochs now.\n",
    "- Running at 15 epochs did help in raising accuracy to 67% along with more covolution layers and the normalizing. Now adding BatchNormalization layers to model so that the feature space be kept tight.\n",
    "- Final run was able to raise accuracy to 68%. I could not get it to 85% but if I had more time I might be able to. I was able to get training accuracy to 88% but clearly there is overfitting going on since validation accuracy is much lower. If I had more time, I would experiment with adding some dropout layers into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak5lAXKuzKzG"
   },
   "source": [
    "Useful reading: \n",
    "*   https://www.tensorflow.org/tutorials/generative/style_transfer\n",
    "\n",
    "*   https://towardsdatascience.com/build-your-own-convolution-neural-network-in-5-mins-4217c2cf964f"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
